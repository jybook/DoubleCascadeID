[1;34mgraphnet[0m [MainProcess] [33mWARNING [0m 2025-02-26 15:54:39 - __init__ - GraphNeTDataModule did not receive an argument for `test_selection` and will therefore not have a prediction dataloader available.[0m
[1;34mgraphnet[0m [MainProcess] [32mINFO    [0m 2025-02-26 15:54:39 - __init__ - GraphNeTDataModule did not receive an for `selection`. Selection will will automatically be created with a split of train: 0.9 and validation: 0.1[0m
/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/jbook/spack/var/spack/environments/graphnet/.spack-env/view/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[1;34mgraphnet[0m [MainProcess] [32mINFO    [0m 2025-02-26 15:54:44 - StandardModel.main - EarlyStopping has been added with a patience of 2.[0m
[1;34mgraphnet[0m [MainProcess] [32mINFO    [0m 2025-02-26 15:54:44 - StandardModel.main - Training initiated with callbacks: ProgressBar, EarlyStopping, ModelCheckpoint[0m
/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/jbook/spack/var/spack/environments/graphnet/.spack-env/view/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_estimator.py ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Within easy_model.py:
Length of dataloader:  2239
length of val_dataloader 2240
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/jbook/spack/var/spack/environments/graphnet/.spack-env/view/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "

  | Name              | Type       | Params
-------------------------------------------------
0 | _tasks            | ModuleList | 129
1 | _graph_definition | KNNGraph   | 0
2 | backbone          | DynEdge    | 1.4 M
-------------------------------------------------
1.4 M     Trainable params
0         Non-trainable params
1.4 M     Total params
5.529     Total estimated model params size (MB)
Epoch  0: 100%|[32mâ–ˆ[0m| 2239/2239 [03:03<00:00, 12.21 batch(es)/s, lr=0.00098, val_loss=[0m
/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/jbook/spack/opt/spack/linux-rocky8-icelake/gcc-8.5.0/python-3.11.9-3kjyspb5ggk7awajofefkc4r24aa6rao/lib/python3.11/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.
In addition, using fork() with Python in general is a recipe for mysterious
deadlocks and crashes.

The most likely reason you are seeing this error is because you are using the
multiprocessing module on Linux, which uses fork() by default. This will be
fixed in Python 3.14. Until then, you want to use the "spawn" context instead.

See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.

If you really know what your doing, you can silence this warning with the warning module
or by setting POLARS_ALLOW_FORKING_THREAD=1.

  self.pid = os.fork()
Epoch  1: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 2239/2239 [02:21<00:00, 15.77 batch(es)/s, lr=0.000939, val_loss=426.0, train_loss=428.0][0m
Epoch  2: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 2239/2239 [02:17<00:00, 16.29 batch(es)/s, lr=0.000899, val_loss=426.0, train_loss=427.0][0m
Epoch  3: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 2239/2239 [02:16<00:00, 16.39 batch(es)/s, lr=0.000859, val_loss=425.0, train_loss=426.0][0m
Epoch  4: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 2239/2239 [02:16<00:00, 16.35 batch(es)/s, lr=0.000818, val_loss=426.0, train_loss=425.0][0m
Epoch  5: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 2239/2239 [02:16<00:00, 16.38 batch(es)/s, lr=0.000778, val_loss=424.0, train_loss=425.0][0m
Epoch  6: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 2239/2239 [02:23<00:00, 15.60 batch(es)/s, lr=0.000737, val_loss=424.0, train_loss=424.0][0m
Epoch  7: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 2239/2239 [01:15<00:00, 29.56 batch(es)/s, lr=0.000697, val_loss=424.0, train_loss=424.0][0m
Validation DataLoader 0:  31%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                   [0m| 700/2240 [00:21<00:48, 32.00 batch(es)/s][0m
